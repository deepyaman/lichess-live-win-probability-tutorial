{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ibis\n",
    "import ibis_ml as ml\n",
    "from ibis import _\n",
    "\n",
    "ibis.options.interactive = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's pick up where we left off by reloading our model input table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_input_table = ibis.read_parquet(\"model_input_table.parquet\")\n",
    "model_input_table = model_input_table.drop(\n",
    "    \"elo_diff\"\n",
    ")  # TODO: Remove \"elo_diff\" from feature engineering, and then remove this\n",
    "model_input_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data splitting\n",
    "\n",
    "To get started, let's split this single dataset into two: a _training_ set and a _testing_ set. We'll keep most of the rows in the original dataset (subset chosen randomly) in the _training_ set. The training data will be used to _fit_ the model, and the _testing_ set will be used to measure model performance.\n",
    "\n",
    "Because the order of rows in an Ibis table is undefined, we need a unique key to split the data reproducibly. To ensure that moves corresponding to a particular game aren't split across the _training_ and _testing_ sets, we'll only split by `game_id` (instead of splitting by `game_id` and `ply`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data frames for the two sets:\n",
    "train_data, test_data = ml.train_test_split(\n",
    "    model_input_table,\n",
    "    unique_key=\"game_id\",\n",
    "    # Put 3/4 of the data into the training set\n",
    "    test_size=0.25,\n",
    "    num_buckets=4,\n",
    "    # Set the seed to enable reproducible analysis\n",
    "    random_seed=111,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit and transform `X_train` using a preprocessing recipe\n",
    "`.to_ibis()` transforms the fitted `X_train` before converting it to an ibis table. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lichess_recipe = ml.Recipe(\n",
    "    ml.DropZeroVariance(ml.everything()),\n",
    "    ml.Drop(ml.string()),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train_data.drop(\"target\")\n",
    "y_train = (train_data.target * 2).cast(\n",
    "    int\n",
    ")  # Convert 0.0 (black win), 0.5 (draw), and 1.0 (white win) to [0 1 2] class labels for a classifier\n",
    "X_test = test_data.drop(\"target\")\n",
    "y_test = (test_data.target * 2).cast(int)\n",
    "\n",
    "X_fit_transformed = lichess_recipe.fit(X_train).to_ibis(X_train)\n",
    "X_fit_transformed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "All features need to encoded as numeric datatypes. For LetSQL inference, float works best. How would you cast all columns to float64 at the end of this recipe?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lichess_recipe = ml.Recipe(\n",
    "    ml.DropZeroVariance(ml.everything()),\n",
    "    ml.Drop(ml.string()),\n",
    "    # Add your code here\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lichess_recipe = ml.Recipe(\n",
    "    ml.DropZeroVariance(ml.everything()),\n",
    "    ml.Drop(ml.string()),\n",
    "    ml.Cast(ml.everything(), \"float64\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit a model with a pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "pipe = Pipeline(\n",
    "    [\n",
    "        (\"lichess_recipe\", lichess_recipe),\n",
    "        (\"xgb_clf\", xgb.XGBClassifier(n_estimators=20)),\n",
    "    ]\n",
    ")\n",
    "pipe.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get a sense of our fitted XGBoost model, let's plot feature importance. `importance_type='gain'` plots the average gain of splits using a feature, and `importance_type='cover'` plots the average number of samples impacted by splits using a feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_fit_transformed = pipe[\"lichess_recipe\"].to_ibis(X_train)\n",
    "pipe[\"xgb_clf\"].get_booster().feature_names = X_fit_transformed.columns\n",
    "\n",
    "xgb.plot_importance(\n",
    "    pipe[\"xgb_clf\"], importance_type=\"gain\", xlabel=\"Average Gain\", show_values=False\n",
    ")\n",
    "xgb.plot_importance(\n",
    "    pipe[\"xgb_clf\"],\n",
    "    importance_type=\"cover\",\n",
    "    xlabel=\"Average Coverage (# of samples impacted)\",\n",
    "    show_values=False,\n",
    ")\n",
    "# If you add more features later on, you can use the max_num_features keyword argument\n",
    "# to plot the more important ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Training score: {pipe.score(X_train, y_train)}\")\n",
    "print(f\"Test score: {pipe.score(X_test, y_test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use a trained workflow to predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sys\n",
    "\n",
    "\n",
    "def log_loss(y_true: np.array, y_pred: np.array):\n",
    "    y_true = y_true.astype(\"float64\")\n",
    "    y_pred = y_pred.astype(\"float64\")\n",
    "\n",
    "    y_pred = y_pred.clip(sys.float_info.epsilon, 1 - sys.float_info.epsilon)\n",
    "    log_losses = y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred)\n",
    "\n",
    "    return np.mean(-log_losses)\n",
    "\n",
    "\n",
    "def print_losses(y_true, y_pred, y_train):\n",
    "    print(f\"Log loss: {log_loss(y_true, y_pred)}\")\n",
    "    print(\n",
    "        f\"Log loss of predicting mean of y_train: {log_loss(y_true, y_train.mean()*np.ones_like(y_true))}\"\n",
    "    )\n",
    "    print(f\"Log loss of perfect prediction: {log_loss(y_true, y_true)}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_proba = pipe.predict_proba(X_test)\n",
    "y_pred_win_proba = y_pred_proba[:, 2] + 0.5 * y_pred_proba[:, 1]\n",
    "\n",
    "test_results_df = test_data.select(\"ply\", \"target\").to_pandas()\n",
    "test_results_df[\"y_pred_win\"] = y_pred_win_proba\n",
    "\n",
    "train_results_df = train_data.select(\"ply\", \"target\").to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_losses(\n",
    "    test_results_df.target, test_results_df.y_pred_win, train_results_df.target\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for move in range(0, 60 + 1, 5):\n",
    "    print(f\"Move: {move+1}\")\n",
    "    print_losses(\n",
    "        test_results_df[test_results_df.ply == 2 * move + 1].target,\n",
    "        test_results_df[test_results_df.ply == 2 * move + 1].y_pred_win,\n",
    "        train_results_df[train_results_df.ply == 2 * move + 1].target,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create an interpretable model with logistic regression "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "basic_steps = (\n",
    "    ml.DropZeroVariance(ml.everything()),\n",
    "    ml.Drop(ml.string()),\n",
    "    ml.Cast(ml.everything(), \"float64\"),\n",
    ")\n",
    "lr_steps = (\n",
    "    ml.ImputeMean(ml.numeric()),\n",
    "    ml.ScaleStandard(ml.numeric()),\n",
    ")\n",
    "\n",
    "lr_pipe = Pipeline(\n",
    "    [\n",
    "        (\"lr_recipe\", ml.Recipe(*(basic_steps + lr_steps))),\n",
    "        (\"lr_model\", LogisticRegression()),\n",
    "    ]\n",
    ")\n",
    "lr_pipe.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Logistic regression:\")\n",
    "print(f\"Training score: {lr_pipe.score(X_train, y_train)}\")\n",
    "print(f\"Test score: {lr_pipe.score(X_test, y_test)}\")\n",
    "print()\n",
    "print(\"XGBoost (from above):\")\n",
    "print(f\"Training score: {pipe.score(X_train, y_train)}\")\n",
    "print(f\"Test score: {pipe.score(X_test, y_test)}\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not too shabby for such a simple model! Predicting white win for everything would result in predicting the correct class for 48.2% of the rows in the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_results_df[test_results_df.target > 0.99].shape[0] / test_results_df.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's take a look at the coefficients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "X_fit_transformed = lr_pipe[\"lr_recipe\"].to_ibis(X_train)\n",
    "\n",
    "coef_df = pd.DataFrame(\n",
    "    lr_pipe[\"lr_model\"].coef_,\n",
    "    columns=X_fit_transformed.columns,\n",
    "    index=[\"black win\", \"draw\", \"white win\"],\n",
    ")\n",
    "coef_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All of these make sense to me, except for the coefficients for `white_elo` and `black_elo`, which seem backwards -- a higher rating for white decreases white's probability of winning and (very slightly) increases black's probability of winning, and a higher rating for black decreases black's probability of winning more than it decreases white's probability of winning. This might be because, after accounting for the effects of all of the other features, rating has a counterintuitive effect. To check that we didn't make a mistake while creating our `model_input_table`, let's fit a model using only those two features. By not standardizing the features, I can also apply the coefficients directly to make predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_steps = (ml.Drop(~ml.endswith(\"elo\")),)  # preserve only \"white_elo\" and \"black_elo\"\n",
    "\n",
    "lr_pipe = Pipeline(\n",
    "    [\n",
    "        (\"lr_recipe\", ml.Recipe(*(basic_steps + lr_steps))),\n",
    "        (\"lr_model\", LogisticRegression(penalty=None)),\n",
    "    ]\n",
    ")\n",
    "lr_pipe.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_fit_transformed = lr_pipe[\"lr_recipe\"].to_ibis(X_train)\n",
    "coef_df = pd.DataFrame(\n",
    "    lr_pipe[\"lr_model\"].coef_,\n",
    "    columns=X_fit_transformed.columns,\n",
    "    index=[\"black win\", \"draw\", \"white win\"],\n",
    ")\n",
    "coef_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy as sp\n",
    "\n",
    "beta_matrix = np.hstack(\n",
    "    [lr_pipe[\"lr_model\"].coef_, np.transpose([lr_pipe[\"lr_model\"].intercept_])]\n",
    ")\n",
    "\n",
    "white_ratings = [2800, 2800, 1000, 1000]\n",
    "black_ratings = [1000, 2800, 1000, 2800]\n",
    "\n",
    "for white_rating, black_rating in zip(white_ratings, black_ratings):\n",
    "    print(f\"{white_rating} white, {black_rating} black:\")\n",
    "    print(\n",
    "        sp.special.softmax(\n",
    "            beta_matrix @ np.transpose([[white_rating, black_rating, 1]])\n",
    "        )\n",
    "    )  # @ signifies matrix multiplication\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The effect of rating in this dataset is much weaker than we'd expect. Theoretically, a player rated 200 points higher than their opponent should have an expected outcome (i.e., percent win + 0.5*percent draw) of 76%. But a combination of a high percentage of fast games (blitz and bullet), and the vast majority of the games involving players with ratings within 100 points of each other, might be contributing to making ratings not too useful for predicting win probability here. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Training score: {lr_pipe.score(X_train, y_train)}\")\n",
    "print(f\"Test score: {lr_pipe.score(X_test, y_test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you're curious, here's one of the better logistic regression models I was able to come up with just 4 features (not counting ^3 transformations) -- `mate_eval`, `regular_eval`, `white_adjusted_clock_usage`, and `black_adjusted_clock_usage`. Of course, blinding throwing everything into XGBoost, as we did for our first model, still performs better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_MOVES = 10\n",
    "\n",
    "feature_suffixes = (\"eval\", \"adjusted_clock_usage\")\n",
    "# Not including \"ply\" and \"elo_diff\" makes the coefficients for mate_eval^3 more interpretable\n",
    "# without sacrificing much accuracy.\n",
    "\n",
    "lr_steps = (\n",
    "    ml.Mutate(\n",
    "        adjusted_base_time=_.base_time + _.increment * NUM_MOVES,\n",
    "        white_adjusted_clock=_.white_clock + _.increment * NUM_MOVES,\n",
    "        black_adjusted_clock=_.black_clock + _.increment * NUM_MOVES,\n",
    "    ),\n",
    "    ml.Mutate(\n",
    "        white_adjusted_clock_usage=(_.adjusted_base_time - _.white_adjusted_clock)\n",
    "        / _.adjusted_base_time,\n",
    "        black_adjusted_clock_usage=(_.adjusted_base_time - _.black_adjusted_clock)\n",
    "        / _.adjusted_base_time,\n",
    "    ),\n",
    "    ml.Mutate(elo_diff=_.white_elo - _.black_elo),\n",
    "    # in case you want to play with adding \"elo_diff\" to feature_suffixes above\n",
    "    ml.Drop(~ml.endswith(feature_suffixes)),\n",
    "    ml.ImputeMean(ml.numeric()),\n",
    "    ml.MutateAt(ml.endswith(\"eval\"), pow3=_**3),\n",
    "    ml.ScaleStandard(~ml.contains(\"adjusted_clock_usage\")),\n",
    ")\n",
    "\n",
    "lr_pipe = Pipeline(\n",
    "    [\n",
    "        (\"lr_recipe\", ml.Recipe(*(basic_steps + lr_steps))),\n",
    "        (\"lr_model\", LogisticRegression()),\n",
    "    ]\n",
    ")\n",
    "lr_pipe.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Training score: {lr_pipe.score(X_train, y_train)}\")\n",
    "print(f\"Test score: {lr_pipe.score(X_test, y_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_fit_transformed = lr_pipe[\"lr_recipe\"].to_ibis(X_train)\n",
    "coef_df = pd.DataFrame(\n",
    "    lr_pipe[\"lr_model\"].coef_,\n",
    "    columns=X_fit_transformed.columns,\n",
    "    index=[\"black win\", \"draw\", \"white win\"],\n",
    ")\n",
    "coef_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of game with titled player: 5vD7WOT9\n",
    "test_data[_.game_id == \"5vD7WOT9\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ordered_titles_list = [\n",
    "    \"BOT\",\n",
    "    \"WCM\",\n",
    "    \"WFM\",\n",
    "    \"NM\",\n",
    "    \"CM\",\n",
    "    \"WIM\",\n",
    "    \"FM\",\n",
    "    \"WGM\",\n",
    "    \"IM\",\n",
    "    \"LM\",\n",
    "    \"GM\",\n",
    "]\n",
    "expression = \"_.case()\"\n",
    "\n",
    "for i, title in enumerate(ordered_titles_list):\n",
    "    expression += f\".when('{title}', {i+1})\"\n",
    "\n",
    "expression += \".end()\"\n",
    "expression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lichess_rec = ml.Recipe(\n",
    "    ml.MutateAt(ml.endswith(\"title\"), eval(expression)),\n",
    "    ml.Mutate(elo_diff=_.white_elo - _.black_elo),\n",
    "    ml.DropZeroVariance(ml.everything()),\n",
    "    ml.Drop(ml.string()),\n",
    "    ml.Cast(ml.everything(), \"float64\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = Pipeline(\n",
    "    [(\"lichess_rec\", lichess_rec), (\"xgb_clf\", xgb.XGBClassifier(n_estimators=20))]\n",
    ")\n",
    "# pipe = Pipeline([(\"lichess_rec\", lichess_rec), (\"xgb\", xgb.XGBRegressor(n_estimators=20))])\n",
    "\n",
    "X_train = train_data.drop(\"target\")\n",
    "y_train = train_data.target * 2\n",
    "\n",
    "pipe.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = test_data.drop(\"target\")\n",
    "y_test = test_data.target * 2\n",
    "\n",
    "print(f\"Training score: {pipe.score(X_train, y_train)}\")\n",
    "print(f\"Test score: {pipe.score(X_test, y_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_fit_transformed = pipe[\"lichess_rec\"].to_ibis(X_train)\n",
    "pipe[\"xgb_clf\"].get_booster().feature_names = X_fit_transformed.columns\n",
    "\n",
    "xgb.plot_importance(\n",
    "    pipe[\"xgb_clf\"], importance_type=\"gain\", xlabel=\"Average Gain\", show_values=False\n",
    ")\n",
    "xgb.plot_importance(\n",
    "    pipe[\"xgb_clf\"],\n",
    "    importance_type=\"cover\",\n",
    "    xlabel=\"Average Coverage (# of samples impacted)\",\n",
    "    show_values=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import graphviz\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig = plt.figure(figsize=(60, 60))\n",
    "ax = fig.subplots()\n",
    "xgb.plot_tree(pipe[\"xgb_clf\"], num_trees=20, rankdir=\"LR\", ax=ax)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
